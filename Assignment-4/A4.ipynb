{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# pyspark --packages graphframes:graphframes:0.3.0-spark2.0-s_2.11 --jars graphframes-0.3.0-spark2.0-s_2.11.jar\n",
    "pyspark_submit_args = '--packages graphframes:graphframes-0.8.0-spark3.0-s_2.12 pyspark-shell'# --jars graphframes-0.8.0-spark3.0-s_2.12.jar'\n",
    "pyspark_submit_args = '--jars graphframes-0.8.0-spark3.0-s_2.12.jar pyspark-shell'\n",
    "\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "conf = pyspark.SparkConf()\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "\n",
    "# from pyspark import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import *\n",
    "from pyspark.sql import *\n",
    "from graphframes import *\n",
    "sparkSession = SparkSession.builder.appName('A4').master('local').getOrCreate()\n",
    "# spark = SparkSession.builder.appName('BDA_A2').master('local').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = open('soc-Epinions1.txt', 'r')\n",
    "lines = dataset.readlines()\n",
    "lines = lines[4:] # skipping the comments in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices = set()\n",
    "edges = []\n",
    "for line in lines:\n",
    "    source, dest = line.split()\n",
    "    vertices.add((source,))\n",
    "    vertices.add((dest,))\n",
    "    edges.append((source, dest))\n",
    "#     print(source, dest)\n",
    "#     break\n",
    "vertices = list(vertices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o26.applySchemaToPythonRDD.\n: java.lang.NoClassDefFoundError: scala/Product$class\r\n\tat scala.reflect.internal.Symbols$SymbolOps.<init>(Symbols.scala:3631)\r\n\tat scala.reflect.internal.Symbols$class.$init$(Symbols.scala:3632)\r\n\tat scala.reflect.internal.SymbolTable.<init>(SymbolTable.scala:16)\r\n\tat scala.reflect.runtime.JavaUniverse.<init>(JavaUniverse.scala:16)\r\n\tat scala.reflect.runtime.package$.universe$lzycompute(package.scala:17)\r\n\tat scala.reflect.runtime.package$.universe(package.scala:17)\r\n\tat org.apache.spark.sql.catalyst.ScalaReflection$.<init>(ScalaReflection.scala:48)\r\n\tat org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>(ScalaReflection.scala)\r\n\tat org.apache.spark.sql.catalyst.encoders.RowEncoder$.serializerFor(RowEncoder.scala:77)\r\n\tat org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply(RowEncoder.scala:66)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:88)\r\n\tat org.apache.spark.sql.SparkSession.internalCreateDataFrame(SparkSession.scala:554)\r\n\tat org.apache.spark.sql.SparkSession.applySchemaToPythonRDD(SparkSession.scala:714)\r\n\tat org.apache.spark.sql.SparkSession.applySchemaToPythonRDD(SparkSession.scala:699)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.lang.ClassNotFoundException: scala.Product$class\r\n\tat java.net.URLClassLoader.findClass(Unknown Source)\r\n\tat java.lang.ClassLoader.loadClass(Unknown Source)\r\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Unknown Source)\r\n\tat java.lang.ClassLoader.loadClass(Unknown Source)\r\n\t... 25 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-a739820b104f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvertices_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvertices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0medges_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0medges\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"src\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"dst\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Spark\\python\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    787\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m         \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapplySchemaToPythonRDD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    790\u001b[0m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_schema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Spark\\python\\lib\\py4j-0.10.8.1-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1284\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1285\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1286\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1288\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Spark\\python\\lib\\py4j-0.10.8.1-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o26.applySchemaToPythonRDD.\n: java.lang.NoClassDefFoundError: scala/Product$class\r\n\tat scala.reflect.internal.Symbols$SymbolOps.<init>(Symbols.scala:3631)\r\n\tat scala.reflect.internal.Symbols$class.$init$(Symbols.scala:3632)\r\n\tat scala.reflect.internal.SymbolTable.<init>(SymbolTable.scala:16)\r\n\tat scala.reflect.runtime.JavaUniverse.<init>(JavaUniverse.scala:16)\r\n\tat scala.reflect.runtime.package$.universe$lzycompute(package.scala:17)\r\n\tat scala.reflect.runtime.package$.universe(package.scala:17)\r\n\tat org.apache.spark.sql.catalyst.ScalaReflection$.<init>(ScalaReflection.scala:48)\r\n\tat org.apache.spark.sql.catalyst.ScalaReflection$.<clinit>(ScalaReflection.scala)\r\n\tat org.apache.spark.sql.catalyst.encoders.RowEncoder$.serializerFor(RowEncoder.scala:77)\r\n\tat org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply(RowEncoder.scala:66)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:88)\r\n\tat org.apache.spark.sql.SparkSession.internalCreateDataFrame(SparkSession.scala:554)\r\n\tat org.apache.spark.sql.SparkSession.applySchemaToPythonRDD(SparkSession.scala:714)\r\n\tat org.apache.spark.sql.SparkSession.applySchemaToPythonRDD(SparkSession.scala:699)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.lang.ClassNotFoundException: scala.Product$class\r\n\tat java.net.URLClassLoader.findClass(Unknown Source)\r\n\tat java.lang.ClassLoader.loadClass(Unknown Source)\r\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Unknown Source)\r\n\tat java.lang.ClassLoader.loadClass(Unknown Source)\r\n\t... 25 more\r\n"
     ]
    }
   ],
   "source": [
    "vertices_df = sparkSession.createDataFrame(vertices, [\"id\"])\n",
    "edges_df = sparkSession.createDataFrame(edges, [\"src\", \"dst\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = GraphFrame(vertices_df, edges_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|  id|inDegree|\n",
      "+----+--------+\n",
      "|1572|     180|\n",
      "|2069|      51|\n",
      "| 296|     121|\n",
      "|1512|     131|\n",
      "|2904|      53|\n",
      "|3414|      16|\n",
      "| 829|      63|\n",
      "| 675|      33|\n",
      "|2088|      80|\n",
      "|2162|      61|\n",
      "|4032|      17|\n",
      "|2294|      56|\n",
      "|6613|      59|\n",
      "|6731|       8|\n",
      "|1436|     181|\n",
      "| 691|      47|\n",
      "|9009|      34|\n",
      "|3210|      34|\n",
      "|9586|       1|\n",
      "|9993|       2|\n",
      "+----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "graph.inDegrees.show()\n",
    "alpha = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|  id|          pagerank|\n",
      "+----+------------------+\n",
      "|  18|325.88530464894075|\n",
      "| 737| 212.6657655504572|\n",
      "|1719|147.22687135149994|\n",
      "| 118|145.06701914713187|\n",
      "| 790| 142.3344648404173|\n",
      "| 136|139.79117216951684|\n",
      "| 143| 139.0543177981888|\n",
      "|  40|118.38451283685471|\n",
      "|1619|106.36353372544302|\n",
      "|4415|101.46022715498134|\n",
      "|1179| 98.47148432038729|\n",
      "| 849|   98.249965640328|\n",
      "| 725| 98.18624344474455|\n",
      "|  27| 95.18549998640358|\n",
      "|1401| 93.16161309296348|\n",
      "|  77| 92.67561681223977|\n",
      "| 128| 92.61946884069384|\n",
      "| 401| 92.60553359356562|\n",
      "|1191| 92.21772965271713|\n",
      "| 135| 89.42293162565127|\n",
      "+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = graph.pageRank(resetProbability=1-alpha, maxIter=20)\n",
    "results.vertices.sort('pagerank' , ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://graphframes.github.io/graphframes/docs/_site/api/python/graphframes.html#graphframes.GraphFrame.parallelPersonalizedPageRank\n",
    "results2 = graph.pageRank(resetProbability=1-alpha, maxIter=20, sourceId=\"18\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|  id|            pagerank|\n",
      "+----+--------------------+\n",
      "|  18| 0.21409061657990314|\n",
      "| 118|0.006807206592250928|\n",
      "| 790|0.006075429899075042|\n",
      "| 136|0.005978881253253704|\n",
      "|1191|0.005723130330628862|\n",
      "| 128|0.005716570772041351|\n",
      "|  59|0.005452936546152758|\n",
      "|1909|0.005371970954204318|\n",
      "| 735|0.005201034886617...|\n",
      "|1398|0.005071498194091262|\n",
      "|1910|0.005065724611604955|\n",
      "| 848|0.004906769572804259|\n",
      "|1903|0.004875843633775832|\n",
      "|1894|0.004832714557261...|\n",
      "|1897|0.004739363135899272|\n",
      "| 722|0.004714263330756526|\n",
      "|1905|0.004637281564599612|\n",
      "|1399|0.004628715582848625|\n",
      "|1754|0.004615841362154648|\n",
      "|1716| 0.00456015521041058|\n",
      "+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results2.vertices.sort('pagerank' , ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
